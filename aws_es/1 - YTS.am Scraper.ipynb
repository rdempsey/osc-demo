{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "import random\n",
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas options\n",
    "pd.set_option('max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe for the movie data\n",
    "df_cols = ['Title', 'YTS URL', 'YTS Rating', 'Thumbnail Image URL', 'Movie Image URL', 'Categories',\n",
    "           'Description', 'Likes', 'IMDb Rating', 'Best Rating', 'Rating Count', 'IMDb URL', 'Downloads']\n",
    "movie_data = pd.DataFrame(columns=df_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data for all the movies on the website\n",
    "# There are 434 pages of movies\n",
    "\n",
    "# The browse page URL\n",
    "url_to_scrape = \"https://yts.am/browse-movies\"\n",
    "\n",
    "page = 1\n",
    "total_pages = 434\n",
    "total_movies = 8671\n",
    "\n",
    "processed_count = 0\n",
    "\n",
    "# Track pages that result in errors\n",
    "urls_not_scraped = []\n",
    "\n",
    "for i in range(1,total_pages+1):\n",
    "    # Each subsequent url is structured like this: https://yts.am/browse-movies?page=2\n",
    "    if i > 1:\n",
    "        url_to_scrape = \"https://yts.am/browse-movies?page={}\".format(i)\n",
    "        \n",
    "    # Retrieve the page\n",
    "    page = requests.get(url_to_scrape)\n",
    "    \n",
    "    # Create the soup\n",
    "    soup = BeautifulSoup(page.content, 'lxml')\n",
    "\n",
    "    for movie in soup('div', class_='browse-movie-wrap col-xs-10 col-sm-4 col-md-5 col-lg-4'):\n",
    "        try:\n",
    "            # Extract the movie info\n",
    "            movie_title = movie.find_all('a')[1].text\n",
    "            yts_url = movie.find_all('a')[1].get('href')\n",
    "            yts_rating = movie.h4.text.split(\" / \")[0]\n",
    "            movie_thumbnail_url = movie.img.get('src')\n",
    "            movie_year = movie.div(class_=\"browse-movie-year\")[0].text\n",
    "\n",
    "            # Get the movie detail page & create the soup\n",
    "            movie_details_soup = requests.get(yts_url)\n",
    "            md_soup = BeautifulSoup(movie_details_soup.content, 'lxml')\n",
    "\n",
    "            # Extract the movie data from the details page\n",
    "            movie_image_url = md_soup.find(\"img\", {\"class\": \"img-responsive\"}).get('src')\n",
    "            movie_categories = md_soup.find(\"div\", {\"id\": \"movie-info\"}).find_all('h2')[1].text.split(\" / \")\n",
    "            movie_description = md_soup('p', class_='hidden-sm hidden-md hidden-lg')[0].text.strip()\n",
    "            movie_likes = md_soup.find(\"span\", {\"id\": \"movie-likes\"}).text\n",
    "            imdb_rating = md_soup.find(\"span\", {\"itemprop\": \"ratingValue\"}).text\n",
    "            best_rating = md_soup.find(\"span\", {\"itemprop\": \"bestRating\"}).text\n",
    "            rating_count = md_soup.find(\"span\", {\"itemprop\": \"ratingCount\"}).text\n",
    "            imdb_url = md_soup.find(\"a\", {\"title\": \"IMDb Rating\"}).get('href')\n",
    "            downloads = md_soup.find_all(\"em\")[2].text.split(\" \")[1]\n",
    "\n",
    "            # Add to the movie data dataframe\n",
    "            movie_data.loc[len(movie_data)] = [movie_title, yts_url, yts_rating, movie_thumbnail_url,\n",
    "                                               movie_image_url, movie_categories, movie_description, movie_likes,\n",
    "                                               imdb_rating, best_rating, rating_count, imdb_url, downloads]\n",
    "        except Exception as e:\n",
    "            urls_not_scraped.append(yts_url)\n",
    "            continue\n",
    "\n",
    "        processed_count += 1\n",
    "        \n",
    "        # Show our progress\n",
    "        print('Processing page {} of 434'.format(i),\n",
    "              '{}% Complete'.format(round(processed_count/total_movies*100,2)),\n",
    "              '{} errors'.format(len(urls_not_scraped)), end=\"\\r\")\n",
    "\n",
    "        # Sleep a bit before getting the next page\n",
    "        sleep(random.uniform(2.1,3.9))\n",
    "\n",
    "print(\"Scrape complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data to a CSV File\n",
    "movie_data.to_csv(\"yts_data.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the URLs we didn't scrape\n",
    "urls_not_scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the dataframe to work with\n",
    "md2 = movie_data.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many movies did we get?\n",
    "len(md2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Comprehend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehend = boto3.client(service_name='comprehend', region_name='us-east-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dominant Language\n",
    "def get_language(text):\n",
    "    if text != \"\":\n",
    "        language_resp = comprehend.detect_dominant_language(Text = text)\n",
    "        languages = language_resp['Languages']\n",
    "        language_count = len(languages)\n",
    "        return languages, language_count\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named Entities\n",
    "def get_named_entities(text):\n",
    "    if text != \"\":\n",
    "        entity_resp = comprehend.detect_entities(Text=text, LanguageCode='en')\n",
    "        entities = entity_resp['Entities']\n",
    "        entity_count = len(entities)\n",
    "        return entities, entity_count\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key Phrases\n",
    "def get_key_phrases(text):\n",
    "    if text != \"\":\n",
    "        kp_resp = comprehend.detect_key_phrases(Text=text, LanguageCode='en')\n",
    "        key_phrases = kp_resp['KeyPhrases']\n",
    "        key_phrase_count = len(key_phrases)\n",
    "        return key_phrases, key_phrase_count\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment\n",
    "def get_sentiment(text):\n",
    "    if text != \"\":\n",
    "        sentiment_resp = comprehend.detect_sentiment(Text=text, LanguageCode='en')\n",
    "        sentiment = sentiment_resp['Sentiment']\n",
    "        sentiment_score_mixed = sentiment_resp['SentimentScore']['Mixed']\n",
    "        sentiment_score_negative = sentiment_resp['SentimentScore']['Negative']\n",
    "        sentiment_score_neutral = sentiment_resp['SentimentScore']['Neutral']\n",
    "        sentiment_score_positive = sentiment_resp['SentimentScore']['Positive']\n",
    "        return sentiment, sentiment_score_mixed, sentiment_score_negative, sentiment_score_neutral, sentiment_score_positive\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the language\n",
    "md2['Language'] = md2['Description'].map(get_language)\n",
    "md2.to_csv(\"yts_data_with_language.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the named entities\n",
    "md2['Entities'] = md2['Description'].map(get_named_entities)\n",
    "md2.to_csv(\"yts_data_with_entities.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the key phrases\n",
    "md2['Key Phrases'] = md2['Description'].map(get_key_phrases)\n",
    "md2.to_csv(\"yts_data_with_key_phrases.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sentiment\n",
    "md2['Sentiment'] = md2['Description'].map(get_sentiment)\n",
    "md2.to_csv(\"yts_data_fully_enriched.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = json.loads(md2.head(1).to_json())\n",
    "print(json.dumps(record, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the language data and append to the dataframe\n",
    "lang_codes = list()\n",
    "lang_scores = list()\n",
    "\n",
    "language_data = md2['Language'].tolist()\n",
    "for item in language_data:\n",
    "    try:\n",
    "        lang_codes.append(item[0][0]['LanguageCode'])\n",
    "        lang_scores.append(item[0][0]['Score'])\n",
    "    except:\n",
    "        lang_codes.append(None)\n",
    "        lang_scores.append(None)\n",
    "    \n",
    "md2['Language Code'] = lang_codes\n",
    "md2['Language Score'] = lang_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the sentiment data\n",
    "\n",
    "sentiments = list()\n",
    "mixed_scores = list()\n",
    "negative_scores = list()\n",
    "neutral_scores = list()\n",
    "positive_scores = list()\n",
    "\n",
    "sentiment_data = md2['Sentiment'].tolist()\n",
    "for item in sentiment_data:\n",
    "    try:\n",
    "        sentiments.append(item[0])\n",
    "        mixed_scores.append(item[1])\n",
    "        negative_scores.append(item[2])\n",
    "        neutral_scores.append(item[3])\n",
    "        positive_scores.append(item[4])\n",
    "    except:\n",
    "        sentiments.append(None)\n",
    "        mixed_scores.append(None)\n",
    "        negative_scores.append(None)\n",
    "        neutral_scores.append(None)\n",
    "        positive_scores.append(None)\n",
    "        \n",
    "md2['Sentiment 2'] = sentiments\n",
    "md2['Sentiment Mixed Score'] = mixed_scores\n",
    "md2['Sentiment Negative Score'] = negative_scores\n",
    "md2['Sentiment Neutral Score'] = neutral_scores\n",
    "md2['Sentiment Positive Score'] = positive_scores\n",
    "\n",
    "del md2['Sentiment']\n",
    "md2.rename(columns={'Sentiment 2': 'Sentiment'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = json.loads(md2.head(1).to_json())\n",
    "print(json.dumps(record, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the columns to the correct data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the Downloads column for float conversion\n",
    "def remove_comma(s):\n",
    "    try:\n",
    "        s = s.replace(\",\", \"\")\n",
    "    except:\n",
    "        pass\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to floats\n",
    "cols_to_convert_to_float = ['YTS Rating', 'Likes', 'IMDb Rating', 'Best Rating', 'Rating Count', 'Downloads']\n",
    "\n",
    "for col in cols_to_convert_to_float:\n",
    "    md2[col] = md2[col].apply(remove_comma)\n",
    "    md2[col] = md2[col].astype('float64', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md2.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Final Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final data file\n",
    "md2.to_csv(\"yts_data_fully_enriched_clean.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
